{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN classifier for microscopy images\n",
    "\n",
    "Step 1 image processing\n",
    "\n",
    "Different options are included. \n",
    "\n",
    "1. Downsample\n",
    "2. gridcrop \n",
    "3. cell crop\n",
    "\n",
    "Filter images first\n",
    "1. Quality control\n",
    "2. Filter by neuron count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n",
      "numpy\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/rhalenathomas/GITHUB/DeepLearningCNN_DiseaseStatusClassifier/venv/requirements.txt'  # testing\n",
    "# Open the file in read mode\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the first 2 lines of the file\n",
    "    for i, line in enumerate(file):\n",
    "        if i < 2:  # Print only the first 10 lines\n",
    "            print(line.strip())  # Strip any leading/trailing whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Failed to start the Kernel 'venv (Python 3.9.0)'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Kernel has not been started"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# activate your virtual environment\n",
    "\n",
    "# navigate to Github folder\n",
    "cd /Users/rhalenathomas/GITHUB/DeepLearningCNN_DiseaseStatusClassifier/venv\n",
    "source Scripts/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Failed to start the Kernel 'venv (Python 3.9.0)'. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Kernel has not been started"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /Users/rhalenathomas/GITHUB/DeepLearningCNN_DiseaseStatusClassifier/venv\n",
    "#pip install requirements.txt\n",
    "#pip install tensorflow\n",
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "\n",
    "# read in quality control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic processing Downsample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 create the model and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_model_original.py\n",
    "#\n",
    "# Used to create the original model architecture\n",
    "print(\"testing\")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=3, activation='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv2D(8, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Print the model summary if needed\n",
    "print(model.summary())\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "model.save('models/model_original')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.py\n",
    "#\n",
    "# Used to train models with data generator\n",
    "#\n",
    "# train_path - Paths for preprocessed images\n",
    "# model_start - Path for model to begin training with 'model_original' (SavedModel format)\n",
    "# model_name - Path to save newly trained model\n",
    "\n",
    "# import all the packages\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load all data from a single directory\n",
    "data_path = '/Users/rhalenathomas/Desktop/temp_images/A'\n",
    "all_data_gen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "all_data = all_data_gen.flow_from_directory(data_path, class_mode='categorical', batch_size=64, target_size=(64, 64), shuffle=True, classes=['healthy', 'unhealthy'])\n",
    "\n",
    "\n",
    "# define all the file pathways and parameters\n",
    "\n",
    "train_set_name = 'XCL_CX5A'        # this is the pathway to the data folder that has \"health\" and \"unhealthy\" folders of images\n",
    "model_start_name = 'model_original'   # this is the model we just built in step 2 \n",
    "model_name = 'model_test_CX5A'   # this is the name we will call the model to save it\n",
    "\n",
    "# Define paths\n",
    "model_start_path = '/Users/rhalenathomas/GITHUB/DeepLearningCNN_DiseaseStatusClassifier/models/' + model_start_name\n",
    "model_path = os.path.join(model_start_path, model_name)  # Using os.path.join for file path concatenation\n",
    "\n",
    "steps_per_epoch = 64\n",
    "max_epochs = 5000\n",
    "early_stopping_patience = 100\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    validation_split=0.2,  # Validation split within the training set\n",
    ")\n",
    "\n",
    "datagen_val = ImageDataGenerator()  # Just load the validation it is scaled already\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_gen = datagen_train.flow(train_data, batch_size=64, shuffle=True, subset='training')\n",
    "val_gen = datagen_val.flow(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the sizes of the resulting datasets\n",
    "print(\"Train set size:\", len(train_gen))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "print(\"Test set size:\", len(test_data))\n",
    "\n",
    "\n",
    "\n",
    "class CombinedGen():\n",
    "    def __init__(self, gens):\n",
    "        self.gens = gens\n",
    "        self.shape = (sum([len(g) for g in self.gens]), 0)\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "            for g in self.gens:\n",
    "                yield next(g)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(g) for g in self.gens])\n",
    "\n",
    "# Load saved model and display the model's architecture\n",
    "model = load_model(model_start_path)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "##### load in the pregenerated model and give it the parameters\n",
    "model.save(model_path)\n",
    "\n",
    "def saveListToFile(listname, pathtosave):\n",
    "    file1 = open(pathtosave,\"w\") \n",
    "    for i in listname:\n",
    "        file1.writelines(\"{}\\n\".format(i))    \n",
    "    file1.close() \n",
    "\n",
    "saveListToFile([steps_per_epoch, max_epochs, early_stopping_patience, train_paths, val_paths], model_path + \"parameters.txt\")\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=early_stopping_patience),\n",
    "             ModelCheckpoint(filepath= model_path + '/best_weights.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "\n",
    "K.set_value(model.optimizer.learning_rate, 0.001)\n",
    "\n",
    "\n",
    "cg = CombinedGen(train_gens)\n",
    "cgv = CombinedGen(val_gens)\n",
    "\n",
    "###### Model Training\n",
    "history = model.fit_generator(\n",
    "        cg.generate(),\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=max_epochs,\n",
    "#        callbacks=callbacks,  # Early stopping\n",
    "        validation_data=cgv.generate(),\n",
    "        validation_steps=steps_per_epoch,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "print(\"fit complete\")\n",
    "\n",
    "# Save the current model after training\n",
    "model.save(model_path)\n",
    "\n",
    "# List all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "saveListToFile([history], model_path + \"history.txt\")\n",
    "\n",
    "\n",
    "\n",
    "# Create plots for accuracy and loss during training and save in model folder\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Plot accuracy history and save it into model directory as PDF\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(os.path.join(model_path, 'training_accuracy.pdf'), format='pdf')\n",
    "plt.clf()\n",
    "\n",
    "# Plot loss history and save it into model directory as PDF\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(os.path.join(model_path, 'training_loss.pdf'), format='pdf')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the data splitting input - run the same model with the same settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.py\n",
    "#\n",
    "# Used to train models with data generator\n",
    "#\n",
    "# train_path - Paths for preprocessed images\n",
    "# model_start - Path for model to begin training with 'model_original' (SavedModel format)\n",
    "# model_name - Path to save newly trained model\n",
    "\n",
    "# import all the packages\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Load all data from a single directory\n",
    "\n",
    "print(\"settting arguments\")\n",
    "data_path = '/Users/rhalenathomas/Desktop/temp_images/A'\n",
    "all_data_gen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "all_data = all_data_gen.flow_from_directory(data_path, class_mode='categorical', batch_size=64, target_size=(64, 64), shuffle=True, classes=['healthy', 'unhealthy'])\n",
    "\n",
    "\n",
    "# define all the file pathways and parameters\n",
    "\n",
    "train_set_name = 'XCL_CX5A'        # this is the pathway to the data folder that has \"health\" and \"unhealthy\" folders of images\n",
    "model_start_name = 'model_original'   # this is the model we just built in step 2 \n",
    "model_name = 'model_test_CX5A'   # this is the name we will call the model to save it\n",
    "\n",
    "# Define paths\n",
    "model_start_path = '/Users/rhalenathomas/GITHUB/DeepLearningCNN_DiseaseStatusClassifier/models/' + model_start_name\n",
    "model_path = os.path.join(model_start_path, model_name)  # Using os.path.join for file path concatenation\n",
    "\n",
    "steps_per_epoch = 64\n",
    "max_epochs = 5000\n",
    "early_stopping_patience = 100\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    validation_split=0.2,  # Validation split within the training set\n",
    ")\n",
    "\n",
    "datagen_val = ImageDataGenerator()  # Just load the validation it is scaled already\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_gen = datagen_train.flow(train_data, batch_size=64, shuffle=True, subset='training')\n",
    "val_gen = datagen_val.flow(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check the sizes of the resulting datasets\n",
    "print(\"Train set size:\", len(train_gen))\n",
    "print(\"Validation set size:\", len(val_data))\n",
    "print(\"Test set size:\", len(test_data))\n",
    "\n",
    "\n",
    "\n",
    "class CombinedGen():\n",
    "    def __init__(self, gens):\n",
    "        self.gens = gens\n",
    "        self.shape = (sum([len(g) for g in self.gens]), 0)\n",
    "\n",
    "    def generate(self):\n",
    "        while True:\n",
    "            for g in self.gens:\n",
    "                yield next(g)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum([len(g) for g in self.gens])\n",
    "\n",
    "# Load saved model and display the model's architecture\n",
    "model = load_model(model_start_path)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "##### load in the pregenerated model and give it the parameters\n",
    "model.save(model_path)\n",
    "\n",
    "def saveListToFile(listname, pathtosave):\n",
    "    file1 = open(pathtosave,\"w\") \n",
    "    for i in listname:\n",
    "        file1.writelines(\"{}\\n\".format(i))    \n",
    "    file1.close() \n",
    "\n",
    "saveListToFile([steps_per_epoch, max_epochs, early_stopping_patience, train_paths, val_paths], model_path + \"parameters.txt\")\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=early_stopping_patience),\n",
    "             ModelCheckpoint(filepath= model_path + '/best_weights.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "\n",
    "K.set_value(model.optimizer.learning_rate, 0.001)\n",
    "\n",
    "\n",
    "cg = CombinedGen(train_gens)\n",
    "cgv = CombinedGen(val_gens)\n",
    "\n",
    "###### Model Training\n",
    "history = model.fit_generator(\n",
    "        train_gen,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=callbacks,  # Early stopping\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=steps_per_epoch,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "print(\"fit complete\")\n",
    "\n",
    "# Save the current model after training\n",
    "model.save(model_path)\n",
    "\n",
    "# List all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "saveListToFile([history], model_path + \"history.txt\")\n",
    "\n",
    "\n",
    "\n",
    "# Create plots for accuracy and loss during training and save in model folder\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Plot accuracy history and save it into model directory as PDF\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(os.path.join(model_path, 'training_accuracy.pdf'), format='pdf')\n",
    "plt.clf()\n",
    "\n",
    "# Plot loss history and save it into model directory as PDF\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(os.path.join(model_path, 'training_loss.pdf'), format='pdf')\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing2\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "print(\"testing2\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "684302c300eff0b62fe689c5a68997e188347b7361bba76c4d20b295dbaf8d1b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
